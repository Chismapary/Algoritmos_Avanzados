# Algoritmos Avanzados - ColoraciÃ³n de Grafos con GNN

## ğŸ“‹ DescripciÃ³n General

Sistema completo para resolver el problema de coloraciÃ³n de grafos usando Graph Neural Networks (GNN) y heurÃ­sticas clÃ¡sicas. El proyecto incluye desde la carga de datos hasta la evaluaciÃ³n comprehensiva con benchmarks simples y complejos (SNAP, DIMACS).

## ğŸ¯ Objetivo del Proyecto

Desarrollar y evaluar un modelo GNN que aprenda a ordenar vÃ©rtices de manera Ã³ptima para el algoritmo greedy de coloraciÃ³n de grafos, comparÃ¡ndolo con heurÃ­sticas clÃ¡sicas como Welsh-Powell, DSATUR y Largest Degree First.

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ librerias.ipynb              # Importaciones y configuraciÃ³n inicial
â”œâ”€â”€ data_loader.ipynb            # Carga y preprocesamiento de grafos
â”œâ”€â”€ heuristics.ipynb             # HeurÃ­sticas clÃ¡sicas de coloraciÃ³n
â”œâ”€â”€ gnn_model.ipynb              # Arquitectura del modelo GNN
â”œâ”€â”€ training.ipynb               # Entrenamiento del modelo
â”œâ”€â”€ evaluation.ipynb             # EvaluaciÃ³n bÃ¡sica
â”œâ”€â”€ benchmark_loader.ipynb       # Carga de benchmarks (SNAP/DIMACS)
â”œâ”€â”€ evaluation_comprehensive.ipynb  # EvaluaciÃ³n completa multi-dataset
â”œâ”€â”€ ejemplo_evaluacion_rapida.ipynb # Tutorial rÃ¡pido
â””â”€â”€ README.md                    # Esta guÃ­a
```

## ğŸš€ Flujo de Trabajo Completo

### 1ï¸âƒ£ **ConfiguraciÃ³n Inicial**
```python
# Ejecutar primero
%run librerias.ipynb
```
Carga todas las dependencias: NetworkX, PyTorch, PyTorch Geometric, NumPy, Pandas, etc.

### 2ï¸âƒ£ **Carga de Datos**
```python
%run data_loader.ipynb
```
**Pipeline de procesamiento:**
- Carga grafo crudo (simulado o real)
- NormalizaciÃ³n (elimina lazos, duplicados)
- ReindexaciÃ³n de vÃ©rtices (0 a n-1)
- ExtracciÃ³n de features (grado, clustering, k-core)
- NormalizaciÃ³n de features
- ConversiÃ³n a formato PyTorch Geometric

**Output:** Objeto `data` con grafo listo para GNN

### 3ï¸âƒ£ **HeurÃ­sticas ClÃ¡sicas**
```python
%run heuristics.ipynb
```
**Implementaciones incluidas:**
- **Random**: Baseline con orden aleatorio
- **Greedy Natural**: Orden natural de nodos
- **Largest Degree First (LDF)**: Orden por grado descendente
- **Welsh-Powell**: Variante mejorada de LDF
- **DSATUR**: SaturaciÃ³n dinÃ¡mica (estado del arte)
- **Parallel Greedy**: SimulaciÃ³n de paralelismo

**Experimentos:** Ejecuta mÃºltiples repeticiones y calcula estadÃ­sticas

### 4ï¸âƒ£ **Modelo GNN**
```python
%run gnn_model.ipynb
```
**Arquitectura:**
- 2 capas GCNConv (Graph Convolutional Network)
- FunciÃ³n de activaciÃ³n ReLU
- Dropout para regularizaciÃ³n
- Output: Score por nodo para ordenamiento

**Input:** Features de nodos + estructura del grafo  
**Output:** Ordenamiento Ã³ptimo para greedy coloring

### 5ï¸âƒ£ **Entrenamiento**
```python
%run training.ipynb
```
**Proceso:**
- GeneraciÃ³n de grafos de entrenamiento
- FunciÃ³n de pÃ©rdida: Penaliza ordenamientos que resultan en mÃ¡s colores
- Optimizador Adam
- Entrenamiento por Ã©pocas con validaciÃ³n

**Output:** Modelo entrenado listo para evaluaciÃ³n

### 6ï¸âƒ£ **EvaluaciÃ³n BÃ¡sica**
```python
%run evaluation.ipynb
```
Compara el modelo GNN contra greedy baseline en el grafo de entrenamiento.

### 7ï¸âƒ£ **EvaluaciÃ³n Comprehensiva** â­
```python
%run evaluation_comprehensive.ipynb
```
Sistema completo de evaluaciÃ³n con mÃºltiples benchmarks y anÃ¡lisis estadÃ­stico.

## ğŸ¯ Componentes Principales

### 1. **benchmark_loader.ipynb**
Carga y gestiÃ³n de datasets de prueba.

**CaracterÃ­sticas:**
- Grafos simples sintÃ©ticos (Petersen, ciclos, ruedas, completos, etc.)
- Grafos complejos (scale-free, small-world, random regular, etc.)
- Parser para formato DIMACS
- Parser para formato SNAP
- Suites predefinidas de benchmarks

**Uso bÃ¡sico:**
```python
# Cargar suite simple
benchmarks = cargar_benchmark('suite', nivel='simple')

# Cargar suite compleja
benchmarks = cargar_benchmark('suite', nivel='complejo')

# Cargar archivo DIMACS
benchmarks = cargar_benchmark('dimacs', filepath='path/to/file.col')

# Cargar archivo SNAP
benchmarks = cargar_benchmark('snap', filepath='path/to/edges.txt')

# Generar grafo especÃ­fico
benchmarks = cargar_benchmark('complejo', tipo='scale_free', n=200, m=5)
```

### 2. **evaluation_comprehensive.ipynb**
Sistema completo de evaluaciÃ³n con mÃºltiples heurÃ­sticas.

**HeurÃ­sticas evaluadas:**
1. **Random** - Baseline con orden aleatorio
2. **Greedy Natural** - Orden natural de nodos
3. **Largest Degree First** - Orden por grado descendente
4. **Welsh-Powell** - Variante mejorada de LDF
5. **DSATUR** - SaturaciÃ³n dinÃ¡mica (estado del arte)
6. **GNN-guided** - Guiado por red neuronal (cuando disponible)

**MÃ©tricas calculadas:**
- NÃºmero de colores (promedio, std, min, max)
- Tiempo de ejecuciÃ³n
- Validez de la coloraciÃ³n
- EstadÃ­sticas del grafo (nodos, aristas, densidad, etc.)

## ğŸ“ Inicio RÃ¡pido

### OpciÃ³n A: Flujo Completo (Desde Cero)
```python
# 1. ConfiguraciÃ³n
%run librerias.ipynb

# 2. Cargar datos
%run data_loader.ipynb

# 3. Ver heurÃ­sticas clÃ¡sicas
%run heuristics.ipynb

# 4. Definir modelo GNN
%run gnn_model.ipynb

# 5. Entrenar modelo
%run training.ipynb

# 6. Evaluar
%run evaluation.ipynb
```

### OpciÃ³n B: Solo EvaluaciÃ³n de Benchmarks
```python
# Ejecutar directamente
%run ejemplo_evaluacion_rapida.ipynb
```

## ğŸ§ª Sistema de EvaluaciÃ³n con Benchmarks

### Nivel 1: EvaluaciÃ³n Simple (Comenzar aquÃ­)
```python
# En evaluation_comprehensive.ipynb, ejecutar secciÃ³n 8
benchmarks_simple = cargar_benchmark('suite', nivel='simple')
df_resultados = evaluar_suite_completa(benchmarks_simple, model=None, repeticiones=3)
```

**Benchmarks incluidos:**
- Grafo de Petersen (10 nodos)
- Ciclo de 20 nodos
- Rueda de 15 nodos
- Completo K10
- Bipartito K10,10
- Ãrbol de 30 nodos
- Random 30 nodos

**Tiempo estimado:** 1-2 minutos

### Nivel 2: EvaluaciÃ³n Media
```python
# Ejecutar secciÃ³n 9
benchmarks_medio = cargar_benchmark('suite', nivel='medio')
df_resultados_medio = evaluar_suite_completa(benchmarks_medio, model=None, repeticiones=3)
```

**Benchmarks incluidos:**
- Scale-free (100 nodos)
- Small-world (100 nodos)
- Random regular (100 nodos)
- Geometric (100 nodos)
- Powerlaw cluster (100 nodos)
- Random ErdÅ‘s-RÃ©nyi (100 nodos)

**Tiempo estimado:** 5-10 minutos

### Nivel 3: EvaluaciÃ³n Compleja
```python
# Ejecutar secciÃ³n 10
benchmarks_complejo = cargar_benchmark('suite', nivel='complejo')
df_resultados_complejo = evaluar_suite_completa(benchmarks_complejo, model=None, repeticiones=3)
```

**Benchmarks incluidos:**
- Scale-free (500 nodos)
- Small-world (500 nodos)
- Random regular (500 nodos)
- Random ErdÅ‘s-RÃ©nyi (500 nodos)
- Powerlaw cluster (500 nodos)

**Tiempo estimado:** 20-30 minutos

## ğŸ“Š InterpretaciÃ³n de Resultados

### AnÃ¡lisis EstadÃ­stico
El sistema genera automÃ¡ticamente:

1. **Resumen por mÃ©todo:**
   - Media y desviaciÃ³n estÃ¡ndar de colores
   - MÃ­nimo y mÃ¡ximo de colores
   - Tiempo promedio de ejecuciÃ³n

2. **Ranking de mÃ©todos:**
   - Ordenados por nÃºmero promedio de colores
   - Identifica el mejor mÃ©todo general

3. **ComparaciÃ³n por grafo:**
   - Resultados detallados para cada benchmark
   - ComparaciÃ³n con cotas teÃ³ricas (cuando disponibles)
   - Mejor mÃ©todo para cada instancia

### Visualizaciones
GrÃ¡ficos generados automÃ¡ticamente:

1. **ComparaciÃ³n de mÃ©todos** - Barras horizontales con error bars
2. **Tiempo de ejecuciÃ³n** - ComparaciÃ³n de eficiencia
3. **Escalabilidad** - Nodos vs colores usados
4. **Densidad vs colores** - Comportamiento segÃºn estructura

## ğŸ”¬ MetodologÃ­a CientÃ­fica

### Para un paper o reporte formal:

1. **Ejecutar nivel simple** - Validar implementaciÃ³n
2. **Ejecutar nivel medio** - Obtener resultados principales
3. **Ejecutar nivel complejo** - Demostrar escalabilidad
4. **Agregar DIMACS/SNAP** - Comparar con literatura

### Estructura de reporte sugerida:

```
1. IntroducciÃ³n
   - Problema de coloraciÃ³n de grafos
   - HeurÃ­sticas clÃ¡sicas vs GNN

2. MetodologÃ­a
   - Benchmarks utilizados (simple/medio/complejo)
   - HeurÃ­sticas evaluadas
   - MÃ©tricas de evaluaciÃ³n

3. Resultados Experimentales
   - Tabla resumen por nivel
   - GrÃ¡ficos comparativos
   - AnÃ¡lisis estadÃ­stico

4. DiscusiÃ³n
   - Mejor mÃ©todo por tipo de grafo
   - Trade-offs tiempo vs calidad
   - Escalabilidad

5. Conclusiones
```

## ğŸ“ Archivos Generados

DespuÃ©s de ejecutar la evaluaciÃ³n:

- `resultados_simple.csv` - Resultados nivel simple
- `resultados_medio.csv` - Resultados nivel medio
- `resultados_complejo.csv` - Resultados nivel complejo
- `resultados_evaluacion.png` - Visualizaciones

## ğŸ”§ PersonalizaciÃ³n

### Agregar tu propio benchmark:

```python
# OpciÃ³n 1: Desde archivo DIMACS
mi_grafo = cargar_benchmark('dimacs', filepath='mi_grafo.col', nombre='MiGrafo')

# OpciÃ³n 2: Desde archivo SNAP
mi_grafo = cargar_benchmark('snap', filepath='mi_red.txt', nombre='MiRed')

# OpciÃ³n 3: Generar sintÃ©tico
mi_grafo = cargar_benchmark('complejo', tipo='scale_free', n=300, m=4)

# Evaluar
resultados = evaluar_todas_heuristicas(mi_grafo[0][0], model=None, repeticiones=5)
```

### Modificar nÃºmero de repeticiones:

```python
# MÃ¡s repeticiones = resultados mÃ¡s robustos pero mÃ¡s tiempo
df_resultados = evaluar_suite_completa(benchmarks, model=None, repeticiones=10)
```

### Evaluar con modelo GNN:

```python
# Cargar modelo entrenado
%run training.ipynb

# Evaluar con GNN
df_resultados = evaluar_suite_completa(benchmarks, model=model, repeticiones=5)
```

## ğŸ“š Datasets Externos Recomendados

### DIMACS Challenge Benchmarks
Disponibles en: https://mat.tepper.cmu.edu/COLOR/instances.html

Ejemplos clÃ¡sicos:
- `queen5_5.col` - Grafo de reinas 5x5
- `myciel3.col` - Grafo de Mycielski
- `anna.col` - Registro de Anna Karenina
- `david.col` - Grafo de David
- `games120.col` - Juegos de 120 equipos

### SNAP Datasets
Disponibles en: http://snap.stanford.edu/data/

Redes reales:
- `facebook_combined.txt` - Red social Facebook
- `email-Enron.txt` - Red de emails Enron
- `ca-GrQc.txt` - Colaboraciones en fÃ­sica
- `wiki-Vote.txt` - Red de votaciones Wikipedia

### CÃ³mo usar:

1. Descargar el archivo
2. Colocarlo en una carpeta `datasets/`
3. Cargar con:
```python
grafo_dimacs = cargar_benchmark('dimacs', filepath='datasets/queen5_5.col')
grafo_snap = cargar_benchmark('snap', filepath='datasets/facebook_combined.txt')
```

## âš¡ Tips de Rendimiento

1. **Empezar con nivel simple** - Validar que todo funciona
2. **Usar menos repeticiones** en grafos grandes (repeticiones=1 o 2)
3. **DSATUR es lento** en grafos grandes (>1000 nodos)
4. **Guardar resultados** frecuentemente con `.to_csv()`
5. **Ejecutar por partes** - No necesitas correr todo de una vez

## ğŸ“ Para Tesis o Proyecto

### Checklist de evaluaciÃ³n completa:

- [ ] Ejecutar suite simple (validaciÃ³n)
- [ ] Ejecutar suite medio (resultados principales)
- [ ] Ejecutar suite complejo (escalabilidad)
- [ ] Agregar 3-5 benchmarks DIMACS
- [ ] Agregar 2-3 redes reales SNAP
- [ ] Generar todas las visualizaciones
- [ ] Calcular estadÃ­sticas significativas
- [ ] Documentar configuraciÃ³n experimental
- [ ] Guardar todos los CSV de resultados
- [ ] Incluir grÃ¡ficos en el documento

## ğŸ“ PrÃ³ximos Pasos

### Para Comenzar:
1. **Ejecuta `ejemplo_evaluacion_rapida.ipynb`** - Tutorial interactivo
2. **Revisa el flujo completo** ejecutando notebooks 1-6 en orden
3. **Ejecuta `evaluation_comprehensive.ipynb`** para evaluaciÃ³n completa

### Para Profundizar:
4. **Descarga benchmarks DIMACS/SNAP** y agrÃ©galos a la evaluaciÃ³n
5. **Ajusta hiperparÃ¡metros** del modelo GNN
6. **Genera visualizaciones** para tu reporte/tesis
7. **Compara con literatura** usando benchmarks estÃ¡ndar

## ğŸ”— Referencias

- **DIMACS Benchmarks**: https://mat.tepper.cmu.edu/COLOR/instances.html
- **SNAP Datasets**: http://snap.stanford.edu/data/
- **PyTorch Geometric**: https://pytorch-geometric.readthedocs.io/

## ğŸ“ Notas Importantes

- Los notebooks estÃ¡n diseÃ±ados para ejecutarse en orden secuencial
- Cada notebook carga sus dependencias con `%run`
- Los resultados se guardan automÃ¡ticamente en CSV
- Las visualizaciones se exportan como PNG

Â¡Buena suerte con tu proyecto! ğŸš€
