# GuÃ­a de EvaluaciÃ³n Comprehensiva

## ğŸ“‹ DescripciÃ³n General

Este sistema de evaluaciÃ³n te permite probar algoritmos de coloraciÃ³n de grafos con una metodologÃ­a rigurosa, desde ejemplos simples hasta benchmarks complejos tipo SNAP y DIMACS.

## ğŸ¯ Componentes Principales

### 1. **benchmark_loader.ipynb**
Carga y gestiÃ³n de datasets de prueba.

**CaracterÃ­sticas:**
- Grafos simples sintÃ©ticos (Petersen, ciclos, ruedas, completos, etc.)
- Grafos complejos (scale-free, small-world, random regular, etc.)
- Parser para formato DIMACS
- Parser para formato SNAP
- Suites predefinidas de benchmarks

**Uso bÃ¡sico:**
```python
# Cargar suite simple
benchmarks = cargar_benchmark('suite', nivel='simple')

# Cargar suite compleja
benchmarks = cargar_benchmark('suite', nivel='complejo')

# Cargar archivo DIMACS
benchmarks = cargar_benchmark('dimacs', filepath='path/to/file.col')

# Cargar archivo SNAP
benchmarks = cargar_benchmark('snap', filepath='path/to/edges.txt')

# Generar grafo especÃ­fico
benchmarks = cargar_benchmark('complejo', tipo='scale_free', n=200, m=5)
```

### 2. **evaluation_comprehensive.ipynb**
Sistema completo de evaluaciÃ³n con mÃºltiples heurÃ­sticas.

**HeurÃ­sticas evaluadas:**
1. **Random** - Baseline con orden aleatorio
2. **Greedy Natural** - Orden natural de nodos
3. **Largest Degree First** - Orden por grado descendente
4. **Welsh-Powell** - Variante mejorada de LDF
5. **DSATUR** - SaturaciÃ³n dinÃ¡mica (estado del arte)
6. **GNN-guided** - Guiado por red neuronal (cuando disponible)

**MÃ©tricas calculadas:**
- NÃºmero de colores (promedio, std, min, max)
- Tiempo de ejecuciÃ³n
- Validez de la coloraciÃ³n
- EstadÃ­sticas del grafo (nodos, aristas, densidad, etc.)

## ğŸš€ Flujo de Trabajo Recomendado

### Nivel 1: EvaluaciÃ³n Simple (Comenzar aquÃ­)
```python
# En evaluation_comprehensive.ipynb, ejecutar secciÃ³n 8
benchmarks_simple = cargar_benchmark('suite', nivel='simple')
df_resultados = evaluar_suite_completa(benchmarks_simple, model=None, repeticiones=3)
```

**Benchmarks incluidos:**
- Grafo de Petersen (10 nodos)
- Ciclo de 20 nodos
- Rueda de 15 nodos
- Completo K10
- Bipartito K10,10
- Ãrbol de 30 nodos
- Random 30 nodos

**Tiempo estimado:** 1-2 minutos

### Nivel 2: EvaluaciÃ³n Media
```python
# Ejecutar secciÃ³n 9
benchmarks_medio = cargar_benchmark('suite', nivel='medio')
df_resultados_medio = evaluar_suite_completa(benchmarks_medio, model=None, repeticiones=3)
```

**Benchmarks incluidos:**
- Scale-free (100 nodos)
- Small-world (100 nodos)
- Random regular (100 nodos)
- Geometric (100 nodos)
- Powerlaw cluster (100 nodos)
- Random ErdÅ‘s-RÃ©nyi (100 nodos)

**Tiempo estimado:** 5-10 minutos

### Nivel 3: EvaluaciÃ³n Compleja
```python
# Ejecutar secciÃ³n 10
benchmarks_complejo = cargar_benchmark('suite', nivel='complejo')
df_resultados_complejo = evaluar_suite_completa(benchmarks_complejo, model=None, repeticiones=3)
```

**Benchmarks incluidos:**
- Scale-free (500 nodos)
- Small-world (500 nodos)
- Random regular (500 nodos)
- Random ErdÅ‘s-RÃ©nyi (500 nodos)
- Powerlaw cluster (500 nodos)

**Tiempo estimado:** 20-30 minutos

## ğŸ“Š InterpretaciÃ³n de Resultados

### AnÃ¡lisis EstadÃ­stico
El sistema genera automÃ¡ticamente:

1. **Resumen por mÃ©todo:**
   - Media y desviaciÃ³n estÃ¡ndar de colores
   - MÃ­nimo y mÃ¡ximo de colores
   - Tiempo promedio de ejecuciÃ³n

2. **Ranking de mÃ©todos:**
   - Ordenados por nÃºmero promedio de colores
   - Identifica el mejor mÃ©todo general

3. **ComparaciÃ³n por grafo:**
   - Resultados detallados para cada benchmark
   - ComparaciÃ³n con cotas teÃ³ricas (cuando disponibles)
   - Mejor mÃ©todo para cada instancia

### Visualizaciones
GrÃ¡ficos generados automÃ¡ticamente:

1. **ComparaciÃ³n de mÃ©todos** - Barras horizontales con error bars
2. **Tiempo de ejecuciÃ³n** - ComparaciÃ³n de eficiencia
3. **Escalabilidad** - Nodos vs colores usados
4. **Densidad vs colores** - Comportamiento segÃºn estructura

## ğŸ”¬ MetodologÃ­a CientÃ­fica

### Para un paper o reporte formal:

1. **Ejecutar nivel simple** - Validar implementaciÃ³n
2. **Ejecutar nivel medio** - Obtener resultados principales
3. **Ejecutar nivel complejo** - Demostrar escalabilidad
4. **Agregar DIMACS/SNAP** - Comparar con literatura

### Estructura de reporte sugerida:

```
1. IntroducciÃ³n
   - Problema de coloraciÃ³n de grafos
   - HeurÃ­sticas clÃ¡sicas vs GNN

2. MetodologÃ­a
   - Benchmarks utilizados (simple/medio/complejo)
   - HeurÃ­sticas evaluadas
   - MÃ©tricas de evaluaciÃ³n

3. Resultados Experimentales
   - Tabla resumen por nivel
   - GrÃ¡ficos comparativos
   - AnÃ¡lisis estadÃ­stico

4. DiscusiÃ³n
   - Mejor mÃ©todo por tipo de grafo
   - Trade-offs tiempo vs calidad
   - Escalabilidad

5. Conclusiones
```

## ğŸ“ Archivos Generados

DespuÃ©s de ejecutar la evaluaciÃ³n:

- `resultados_simple.csv` - Resultados nivel simple
- `resultados_medio.csv` - Resultados nivel medio
- `resultados_complejo.csv` - Resultados nivel complejo
- `resultados_evaluacion.png` - Visualizaciones

## ğŸ”§ PersonalizaciÃ³n

### Agregar tu propio benchmark:

```python
# OpciÃ³n 1: Desde archivo DIMACS
mi_grafo = cargar_benchmark('dimacs', filepath='mi_grafo.col', nombre='MiGrafo')

# OpciÃ³n 2: Desde archivo SNAP
mi_grafo = cargar_benchmark('snap', filepath='mi_red.txt', nombre='MiRed')

# OpciÃ³n 3: Generar sintÃ©tico
mi_grafo = cargar_benchmark('complejo', tipo='scale_free', n=300, m=4)

# Evaluar
resultados = evaluar_todas_heuristicas(mi_grafo[0][0], model=None, repeticiones=5)
```

### Modificar nÃºmero de repeticiones:

```python
# MÃ¡s repeticiones = resultados mÃ¡s robustos pero mÃ¡s tiempo
df_resultados = evaluar_suite_completa(benchmarks, model=None, repeticiones=10)
```

### Evaluar con modelo GNN:

```python
# Cargar modelo entrenado
%run training.ipynb

# Evaluar con GNN
df_resultados = evaluar_suite_completa(benchmarks, model=model, repeticiones=5)
```

## ğŸ“š Datasets Externos Recomendados

### DIMACS Challenge Benchmarks
Disponibles en: https://mat.tepper.cmu.edu/COLOR/instances.html

Ejemplos clÃ¡sicos:
- `queen5_5.col` - Grafo de reinas 5x5
- `myciel3.col` - Grafo de Mycielski
- `anna.col` - Registro de Anna Karenina
- `david.col` - Grafo de David
- `games120.col` - Juegos de 120 equipos

### SNAP Datasets
Disponibles en: http://snap.stanford.edu/data/

Redes reales:
- `facebook_combined.txt` - Red social Facebook
- `email-Enron.txt` - Red de emails Enron
- `ca-GrQc.txt` - Colaboraciones en fÃ­sica
- `wiki-Vote.txt` - Red de votaciones Wikipedia

### CÃ³mo usar:

1. Descargar el archivo
2. Colocarlo en una carpeta `datasets/`
3. Cargar con:
```python
grafo_dimacs = cargar_benchmark('dimacs', filepath='datasets/queen5_5.col')
grafo_snap = cargar_benchmark('snap', filepath='datasets/facebook_combined.txt')
```

## âš¡ Tips de Rendimiento

1. **Empezar con nivel simple** - Validar que todo funciona
2. **Usar menos repeticiones** en grafos grandes (repeticiones=1 o 2)
3. **DSATUR es lento** en grafos grandes (>1000 nodos)
4. **Guardar resultados** frecuentemente con `.to_csv()`
5. **Ejecutar por partes** - No necesitas correr todo de una vez

## ğŸ“ Para Tesis o Proyecto

### Checklist de evaluaciÃ³n completa:

- [ ] Ejecutar suite simple (validaciÃ³n)
- [ ] Ejecutar suite medio (resultados principales)
- [ ] Ejecutar suite complejo (escalabilidad)
- [ ] Agregar 3-5 benchmarks DIMACS
- [ ] Agregar 2-3 redes reales SNAP
- [ ] Generar todas las visualizaciones
- [ ] Calcular estadÃ­sticas significativas
- [ ] Documentar configuraciÃ³n experimental
- [ ] Guardar todos los CSV de resultados
- [ ] Incluir grÃ¡ficos en el documento

## ğŸ“ PrÃ³ximos Pasos

1. **Ejecuta `evaluation_comprehensive.ipynb` secciÃ³n 8** para ver el sistema en acciÃ³n
2. **Revisa los resultados** en `resultados_simple.csv`
3. **Analiza las visualizaciones** generadas
4. **Escala a nivel medio** cuando estÃ©s listo
5. **Agrega benchmarks externos** para comparaciÃ³n con literatura

Â¡Buena suerte con tu evaluaciÃ³n! ğŸš€
